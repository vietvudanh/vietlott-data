#!/usr/bin/env python
"""
README Generator for Vietlott Data Project

This script generates a comprehensive README.md file for the GitHub repository
frontpage, including data statistics, predictions, and project information.
"""

from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import polars as pl
from loguru import logger

from vietlott.config.products import get_config
from vietlott.model.strategy.random_strategy import RandomModel


class ReadmeTemplates:
    """Container for README template strings and formatting."""

    @staticmethod
    def get_header() -> str:
        """Get the main header with badges and description."""
        return """# ğŸ° Vietlott Data

[![GitHub Actions](https://github.com/vietvudanh/vietlott-data/workflows/crawl/badge.svg)](https://github.com/vietvudanh/vietlott-data/actions)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Data Updated](https://img.shields.io/badge/data-daily%20updated-brightgreen.svg)](https://github.com/vietvudanh/vietlott-data/commits/main)

> ğŸ“Š **Automated Vietnamese Lottery Data Collection & Analysis**
> 
> This project automatically crawls and analyzes Vietnamese lottery data from [vietlott.vn](https://vietlott.vn/), providing comprehensive statistics and insights for all major lottery products.

## ğŸ¯ Supported Lottery Products

| Product | Link | Description |
|---------|------|-------------|
| **Power 6/55** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/655) | Choose 6 numbers from 1-55 |
| **Power 6/45** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/645) | Choose 6 numbers from 1-45 |
| **Power 5/35** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/535) | Choose 5 numbers from 1-35 |
| **Keno** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/winning-number-keno) | Fast-pace number game |
| **Max 3D** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/max-3d) | 3-digit lottery game |
| **Max 3D Pro** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/max-3dpro) | Enhanced 3D lottery |
| **Bingo18** | [ğŸ”— Results](https://vietlott.vn/vi/trung-thuong/ket-qua-trung-thuong/winning-number-bingo18) | 3 numbers from 0-9 game |
"""

    @staticmethod
    def get_toc() -> str:
        """Get table of contents."""
        return """## ğŸ“‹ Table of Contents

- [ğŸ¯ Supported Lottery Products](#-supported-lottery-products)
- [ğŸ“Š Data Statistics](#-data-statistics)
- [ğŸ”® Prediction Models](#-prediction-models)
- [ğŸ“ˆ Power 6/55 Analysis](#-power-655-analysis)
  - [ğŸ“… Recent Results](#-recent-results)
  - [ğŸ² Number Frequency (All Time)](#-number-frequency-all-time)
  - [ğŸ“Š Frequency Analysis by Period](#-frequency-analysis-by-period)
- [ğŸ“ˆ Power 5/35 Analysis](#-power-535-analysis)
  - [ğŸ“… Recent Results](#-recent-results-1)
  - [ğŸ² Number Frequency (All Time)](#-number-frequency-all-time-1)
  - [ğŸ“Š Frequency Analysis by Period](#-frequency-analysis-by-period-1)
- [âš™ï¸ How It Works](#ï¸-how-it-works)
- [ğŸš€ Installation & Usage](#-installation--usage)
- [ğŸ“„ License](#-license)
"""

    @staticmethod
    def get_how_it_works() -> str:
        """Get how it works section."""
        return """## âš™ï¸ How It Works

### ğŸ¤– Automated Data Collection

This project runs completely automatically using **GitHub Actions** - no server required!

- **â° Schedule**: Runs daily via [GitHub Actions workflow](.github/workflows/crawl.yaml)
- **ğŸ”„ Process**: Fetches latest results â†’ Processes data â†’ Commits to repository
- **ğŸ“Š Analysis**: Generates statistics and updates README automatically

### ğŸ•µï¸ Data Crawling Method

The data collection works by:
1. **ğŸ” Network Analysis**: Inspecting browser-server communication
2. **ğŸ Python Replication**: Recreating the data fetch logic in Python
3. **ğŸ“‹ Structured Storage**: Saving results in JSONL format for easy analysis
4. **ğŸ”„ Continuous Updates**: Daily automated runs ensure fresh data

> **Note**: This is purely for educational and research purposes. No gambling advice is provided.
"""

    @staticmethod
    def get_install_section() -> str:
        """Get installation section."""
        return """## ğŸš€ Installation & Usage

### ğŸ“¦ Install via pip

```bash
pip install -i https://test.pypi.org/simple/ vietlott-data==0.1.3
```

### ğŸ’» Command Line Interface

#### ğŸ” Crawl Data

```bash
vietlott-crawl [OPTIONS] PRODUCT

# Options:
#   --run-date TEXT       Specific date to crawl
#   --index_from INTEGER  Starting page index
#   --index_to INTEGER    Ending page index
#   --help               Show help message
```

#### ğŸ”§ Backfill Missing Data

```bash
vietlott-missing [OPTIONS] PRODUCT

# Options:
#   --limit INTEGER  Number of pages to process
#   --help          Show help message
```

### ğŸ› ï¸ Development Setup

```bash
# Clone the repository
git clone https://github.com/vietvudanh/vietlott-data.git
cd vietlott-data

# Install dependencies
pip install -r requirements-dev.txt

# Run tests
pytest
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">
  <strong>â­ If you find this project useful, please consider giving it a star!</strong>
</div>
"""


class ReadmeGenerator:
    """Main class for generating the README.md file."""

    def __init__(self):
        self.templates = ReadmeTemplates()

    def _balance_long_df(self, df_: pl.DataFrame, n_splits: int = 20) -> pl.DataFrame:
        """Convert long dataframe to multiple columns for better display."""
        if df_.is_empty():
            return df_

        # Convert to pandas for this complex operation that's mainly for display
        import pandas as pd

        df_pd = df_.to_pandas()

        # reset_index may return a view in some edge-cases; work on an explicit copy
        df_pd = df_pd.reset_index().copy()
        # Create converted columns in a separate DataFrame and concat them back to
        # avoid assigning object-dtype data into existing numeric columns which
        # can trigger FutureWarnings about incompatible dtype setting.
        if "result" in df_pd.columns and "count" in df_pd.columns:
            converted = pd.DataFrame(
                {
                    "result": df_pd["result"].apply(lambda x: str(x)).astype(object),
                    "count": df_pd["count"].apply(lambda x: str(x)).astype(object),
                },
                index=df_pd.index,
            )
            # drop original columns then concat converted ones to preserve order
            left = df_pd.drop(columns=["result", "count"])
            df_pd = pd.concat([left, converted], axis=1)

        final = None

        for i in range(len(df_pd) // n_splits + 1):
            dd = df_pd.iloc[i * n_splits : (i + 1) * n_splits]

            if dd.empty:
                continue

            if final is None:
                final = dd
            else:
                final = pd.concat(
                    [
                        final.reset_index(drop=True),
                        pd.DataFrame([None] * len(dd), columns=["-"]),
                        dd.reset_index(drop=True),
                    ],
                    axis="columns",
                )

        if final is not None:
            # ensure we operate on an explicit copy before filling
            final = final.copy().fillna("")
            # Convert back to polars
            return pl.from_pandas(final)
        else:
            return pl.DataFrame()

    def _load_lottery_data(self, product: str) -> pl.DataFrame:
        """Load and prepare lottery data for analysis."""
        try:
            df = pl.read_ndjson(get_config(product).raw_path)

            # Normalize/parse date column which can be in multiple formats
            if "date" in df.columns:
                # Try to parse the date column
                try:
                    # Check if it's already a date type
                    if df["date"].dtype in [pl.Date, pl.Datetime]:
                        df = df.with_columns(pl.col("date").cast(pl.Date))
                    # Check if it's numeric (epoch time)
                    elif df["date"].dtype in [pl.Int64, pl.Int32, pl.Float64]:
                        # Check if values are in milliseconds or seconds
                        max_val = df["date"].max()
                        if max_val > 1_000_000_000_000:
                            # milliseconds
                            df = df.with_columns(
                                (pl.col("date").cast(pl.Int64) / 1000).cast(pl.Datetime("ms")).cast(pl.Date)
                            )
                        else:
                            # seconds
                            df = df.with_columns(pl.col("date").cast(pl.Int64).cast(pl.Datetime("s")).cast(pl.Date))
                    else:
                        # String date - try to parse
                        df = df.with_columns(pl.col("date").str.to_date(strict=False))
                except Exception as e:
                    logger.warning(f"Could not parse date column: {e}")
                    # Fallback: try string parsing
                    df = df.with_columns(pl.col("date").str.to_date(strict=False))

            df = df.sort(["date", "id"], descending=True)
            return df
        except Exception as e:
            logger.error(f"Error loading data for {product}: {e}")
            return pl.DataFrame()

    def _calculate_stats(self, df: pl.DataFrame) -> pl.DataFrame:
        """Calculate number frequency statistics."""
        if df.is_empty():
            return pl.DataFrame()

        df_explode = df.explode("result")
        stats = df_explode.group_by("result").agg(pl.count("id").alias("count"))
        total_count = len(df_explode)
        stats = stats.with_columns(((pl.col("count") / total_count * 100).round(2)).alias("%"))
        return stats

    def _get_data_overview(self) -> str:
        """Generate overview statistics for all products."""
        products = ["power_655", "power_645", "power_535", "keno", "3d", "3d_pro", "bingo18"]
        data_stats = []

        for product in products:
            try:
                df = self._load_lottery_data(product)
                if not df.is_empty():
                    data_stats.append(
                        {
                            "Product": product.replace("_", " ").title(),
                            "Total Draws": df["date"].n_unique(),
                            "Start Date": str(df["date"].min()),
                            "End Date": str(df["date"].max()),
                            "Total Records": df["id"].n_unique(),
                            "First ID": str(df["id"].min()),
                            "Latest ID": str(df["id"].max()),
                        }
                    )
            except Exception as e:
                logger.warning(f"Could not load stats for {product}: {e}")

        if data_stats:
            import pandas as pd

            return pd.DataFrame(data_stats).to_markdown(index=False)
        return "No data available"

    def _generate_predictions_section(self, df: pl.DataFrame) -> str:
        """Generate predictions analysis section."""
        if df.is_empty():
            return "## ğŸ”® Prediction Models\n\n> No data available for predictions.\n"

        try:
            # Convert to pandas for the model (if it expects pandas)
            df_pd = df.to_pandas()

            ticket_per_days = 20
            random_model = RandomModel(df_pd, ticket_per_days)
            random_model.backtest()
            random_model.evaluate()

            df_correct = random_model.df_backtest_evaluate[random_model.df_backtest_evaluate["correct_num"] >= 5][
                ["date", "result", "predicted"]
            ]

            cost_per_day = 10000 * ticket_per_days

            return f"""## ğŸ”® Prediction Models

> âš ï¸ **Disclaimer**: These are experimental models for educational purposes only. Lottery outcomes are random and cannot be predicted reliably.

### ğŸ² Random Strategy Backtest

- **Strategy**: Random number selection
- **Tickets per day**: {ticket_per_days}
- **Daily cost**: {cost_per_day:,} VND
- **Results with 5+ matches**:

{df_correct.to_markdown(index=False) if not df_correct.empty else "No significant matches found in backtest period."}

"""
        except Exception as e:
            logger.error(f"Error generating predictions: {e}")
            return "## ğŸ”® Prediction Models\n\n> Error generating prediction analysis.\n"

    def _generate_power655_analysis(self, df: pl.DataFrame) -> str:
        """Generate detailed Power 6/55 analysis section."""
        if df.is_empty():
            return "## ğŸ“ˆ Power 6/55 Analysis\n\n> No data available for analysis.\n"

        try:
            # Calculate stats for different periods
            stats_all = self._balance_long_df(self._calculate_stats(df))

            current_date = datetime.now().date()
            stats_30d = self._balance_long_df(
                self._calculate_stats(df.filter(pl.col("date") >= (current_date - timedelta(days=30))))
            )
            stats_60d = self._balance_long_df(
                self._calculate_stats(df.filter(pl.col("date") >= (current_date - timedelta(days=60))))
            )
            stats_90d = self._balance_long_df(
                self._calculate_stats(df.filter(pl.col("date") >= (current_date - timedelta(days=90))))
            )

            recent_results = df.head(10)

            # Convert to pandas for markdown display
            import pandas as pd

            recent_results_pd = recent_results.to_pandas()
            stats_all_pd = stats_all.to_pandas() if not stats_all.is_empty() else pd.DataFrame()
            stats_30d_pd = stats_30d.to_pandas() if not stats_30d.is_empty() else pd.DataFrame()
            stats_60d_pd = stats_60d.to_pandas() if not stats_60d.is_empty() else pd.DataFrame()
            stats_90d_pd = stats_90d.to_pandas() if not stats_90d.is_empty() else pd.DataFrame()

            return f"""## ğŸ“ˆ Power 6/55 Analysis

### ğŸ“… Recent Results (Last 10 draws)
{recent_results_pd.to_markdown(index=False)}

### ğŸ² Number Frequency (All Time)
{stats_all_pd.to_markdown(index=False) if not stats_all_pd.empty else "No data available"}

### ğŸ“Š Frequency Analysis by Period

#### Last 30 Days
{stats_30d_pd.to_markdown(index=False) if not stats_30d_pd.empty else "No data available"}

#### Last 60 Days
{stats_60d_pd.to_markdown(index=False) if not stats_60d_pd.empty else "No data available"}

#### Last 90 Days
{stats_90d_pd.to_markdown(index=False) if not stats_90d_pd.empty else "No data available"}

"""
        except Exception as e:
            logger.error(f"Error generating Power 6/55 analysis: {e}")
            return "## ğŸ“ˆ Power 6/55 Analysis\n\n> Error generating analysis.\n"

    # Removed Power 5/35 Analysis section as requested.

    def generate_readme(self) -> str:
        """Generate the complete README content."""
        logger.info("Starting README generation...")

        # Load Power 6/55 data (main focus)
        df_power655 = self._load_lottery_data("power_655")

        # Generate all sections
        header = self.templates.get_header()
        toc = self.templates.get_toc()
        data_overview = self._get_data_overview()
        predictions = self._generate_predictions_section(df_power655)
        power655_analysis = self._generate_power655_analysis(df_power655)
        how_it_works = self.templates.get_how_it_works()
        install_section = self.templates.get_install_section()

        # Combine all sections
        readme_content = f"""{header}

{toc}

## ğŸ“Š Data Statistics

{data_overview}

{predictions}

{power655_analysis}

{how_it_works}

{install_section}
"""

        return readme_content

    def save_readme(self, output_path: Optional[Path] = None) -> None:
        """Generate and save README to file."""
        if output_path is None:
            output_path = Path("./readme.md")

        try:
            readme_content = self.generate_readme()

            with output_path.open("w", encoding="utf-8") as ofile:
                ofile.write(readme_content)

            logger.info(f"README successfully written to {output_path.absolute()}")
        except Exception as e:
            logger.error(f"Error saving README: {e}")
            raise


def main():
    """Main entry point for README generation."""
    try:
        generator = ReadmeGenerator()
        generator.save_readme()
        logger.info("README generation completed successfully!")
    except Exception as e:
        logger.error(f"Failed to generate README: {e}")
        raise


if __name__ == "__main__":
    main()
